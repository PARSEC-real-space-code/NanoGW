#include "../shared/mycomplex.h"
! Weiwei Gao, Feb. 2018
!
! In the density fitting method, the products of wave functions are
! written as linear combinations of interpolation vectors zeta(r).
! See equation 5 in J. Chem. Theory. Comput. 2017, 13, 5420-5431:
!
! \phi_i(r)\psi_j(r) = \sum^{Nu}_{u=1} \zeta_u(r) \phi_i(r_u) \psi_j(r_u)
!
! <==> written in a matrix form: Z = Zeta * C
!  ________________________________________________________________________________
! |  Matrix      |     Shape      |        Contents                                |
! |______________|________________|________________________________________________|
! |   P(r,r_u)   |  Ng * N_u      |    \sum_i \phi_i(r)\psi_i(r_u)                 |
! |              |                |     i=1...Nv, r are full-grid points           |
! |______________|________________|________________________________________________|
! |   Q(r,r_u)   |  Ng * N_u      |    \sum_i \phi_i(r)\psi_i(r_u)                 |
! |              |                |     i=1...Nc, r are full-grid points           |
! |______________|________________|________________________________________________|
! |  Zeta_u(r)   |  Ng *  Nu      |    {..., \zeta_u(r), ...}                      |
! |              |                |     u=1...Nu                                   |
! |______________|________________|________________________________________________|
! |  C(r_u,i,j)  |  Nu * (Nc*Nv)  |    {..., \phi_i(ru)*\psi_j(ru), ...}           |
! |              |                |                                                |
! |______________|________________|________________________________________________|
!  _______________________________________________________________________
! |  Matrix      |                  Parallelization                       |
! |______________|________________________________________________________|
! |   P(r,r_u)   |             Each proc store part of Z                  |
! |              |             P(ngfl, Nc*Nv), ngfl = mydim               |
! |______________|________________________________________________________|
! |   Q(r,r_u)   |             Each proc store part of Z                  |
! |              |             Q(ngfl, Nc*Nv), ngfl = mydim               |
! |______________|________________________________________________________|
! |  Zeta_u(r)   |             Each proc store part of zeta               |
! |              |         zeta(ngfl, Nu), ngfl = mydim*ntrans            |
! |______________|________________________________________________________|
! |  C(r_u,i,j)  |             Every proc store a full copy of C          |
! |              |              Cmtrx ( Nu, Nc*Nv )                       |
! |______________|________________________________________________________|
!
!
! This subroutine calculates the interpolation vectors zeta_u(r)
!
! n_intp   : the number of interpolation vectors or points, or Nu
! n_intp_r : the number of interpolation vectors or points in reduced r-space domain
! intp     : the index of interpolation points in the full grid
! zeta(gvec%nr, n_intp_r) : the interpolation vectors
! kpt%wfn(isp,ikp)%dwf(:,:) : store the wavefunctions \phi_i, \psi_j
!
! For now, this is only partially parallelized
! 1. each processor evaluates part of the matrix Z and part of the matrix C
! 2. collect matrix C from different procs. distribute matrix Z to procs.
! 3. every proc in w_grp solve part of the linear equations to get zeta
!
! This subroutine consists of the following main steps:
!
! Step 1.0: Prepare P(r,r_u)
! Step 1.5: Prepare A=C.C^T=P(r_u,r_v)Q(r_u,r_v), B=Z.C^T=P(r,r_u)Q(r,r_v)
! Step 1.6: DeALLOCATE P and Q
! Step 2.0: solve a linear quation A.zeta=B to get zeta
! Step 3.0: Calculate Mmtrx(1) = <zeta(i)| V_coul(r,r') | zeta(j)> and
!       Mmtrx(2) = <zeta(i)| f^LDA(r) | zeta(j)>
! Step 3.5: Calculate C(r_u,i,j)

! How to reduce memory requirements:
! [ done ]  1. Split zeta into n_slice, and calculate one slice each time, store zeta and
!    vc_zeta in hard drive using HDF5
! [      ]  2. deallocate all the wave functions after using it for constructing PsiV and PsiC
! [      ]  3. store Mmtrx in hard drive using HDF5
! [ working on ]  4. don't calculate Cmtrx, only calculate and store Psi_intp
subroutine Zisdf_parallel_sym_UltraLowMem(gvec, kpt, nspin, isdf_in, kflag, opt, verbose)

#ifdef HIPMAGMA
  use magma
#endif
  use HDF5
  use typedefs
  use mpi_module
  use myconstants
  use fft_module
  use xc_functionals
  implicit none

#ifdef MPI
  include 'mpif.h'
#endif

  type(gspace), intent(in) :: gvec
  type(kptinfo), intent(in) :: kpt
  type(ISDF), intent(inout) :: isdf_in
  !
  ! n_intp_r: number of interpolation points in reduced r-space domain
  ! intp_r(n_intp_r): the index of interpolation points in full grids
  ! ncv(2): the number of wave function pairs for two spins
  !
  ! invpairmap maps the index of pair |cv> to c and v
  integer, intent(in) :: nspin, kflag
  type(options), intent(in) :: opt
  ! If verbose is true, then print out additional debug information
  logical, intent(in) :: verbose
  !
  ! ------------------ Local variables ------------------
  ! P(gvec%mydim, n_intp_r, nspin, kpt%nk)
  ! Q(gvec%mydim, n_intp_r, nspin, kpt%nk)
  ! Cmtrx(n_intp_r, Nc*Nv, nspin, kpt%nk)
  ! Issue need to be addressed: if there are N wfn_grp, does it mean these matrix need to be
  ! calculated by all processors at the same time, or can we distribute the workload later ??
  !
  SCALAR, allocatable :: P(:, :, :), P_intp(:, :, :), &  ! P on reduced domain
                         Q(:, :, :), Q_intp(:, :, :), &  ! Q on reduced domain
                         zeta(:, :, :), vzeta(:, :, :), fzeta(:, :), &
                         tmp_Psi_intp_loc(:, :), tmp_Mmtrx_loc(:, :), &
                         ! matrices and vectors used for solving linear equations
                         Amtrx(:, :, :), Bmtrx(:, :), Xmtrx(:, :), &
                         rho_h(:), rho_h_distr(:, :), &
                         Amtrx_bl(:), Bmtrx_bl(:)
  real(dp), allocatable :: fxc(:, :, :)
  real(dp), allocatable :: tmp_array(:, :, :)  ! (:,:,1): real, (:,:,2): imag
  real(dp) :: qkt(3), tsec(2), norm_factor
  integer, allocatable :: ipiv(:)
  ! counters and temporary integers
  integer :: ipt, ii, jj, iv, ic, irp, jrp, csp, i_row, i_col, lrp1, lrp2, IVV, ICC, JVV, JCC, isp, ikp, errinfo, &
             ipe, n_intp_r, maxncv, maxnv, maxnc, mpi_status(MPI_STATUS_SIZE), n_row, ldn_intp_r, n_cplx
  ! Each processor in a w_grp store part of the wave function
  ! offset: index of grid point from which a processor start to store the wave function
  ! ncount: number of elements of wave functions that a processor stores
  !integer, dimension(0:w_grp%npes-1) :: offset, ncount
  ! temporary dummy variable
  integer :: iptf, iptr, ioff1, ioff2, rank

#ifdef DEBUG
  ! variables for debug and test of accuracy
  character(50) :: dbg_filename = "isdf_dbg.dat"
  integer, parameter :: dbgunit = 20171130
  integer :: outdbg
#endif

  ! external functions
  integer, external :: numroc
  type(xc_type) :: xc_lda
  !
  ! tmporary hdf5 file "zeta_iproc.h5", where iproc is the index of a processor
  ! the following variables is used for temporarily storing zeta and Vcoul_zeta
  !
  character(len=40) :: h5filename, dset_zeta, dset_vczeta
  integer(hid_t) :: file_id, &  ! file identifier
                    dset_zeta_id(nspin, gvec%syms%ntrans), &
                    dset_vczeta_id(nspin, gvec%syms%ntrans), &  ! dataset identifier
                    dspace_zeta, dspace_vczeta, &  ! dataspace identifier
                    subdspace
  integer(hsize_t) :: data_dims(3), subdim(3), shift(3), stride(3), block(3)
  integer :: h5err
  ! scalapack and blacs variables
  integer :: context_system, icntxt_1d, descb_1d(9), info, MB_, mycol_1d, myn_intp_r, myrow_1d, ndim_ipiv, &
             npcol_1d, nprow_1d, RSRC_, desca_1d(9), desca_2d(9), descb_2d(9), nprow_2d, npcol_2d, nbl_2d, &
             ldrowA, ldcolA, myrow_2d, mycol_2d, icntxt_2d, ldrowB, ldcolB

#ifdef DEBUG
  outdbg = 198812 + w_grp%inode
  if (w_grp%master) then
    ! write(*,*) "call isdf_parallel(), write debug info to ", dbg_filename
    open (dbgunit, file=dbg_filename, form='formatted', status='replace')
  end if
#endif
  ! the number of grid points of interpolation vectors zeta(:) stored in each processor
  n_intp_r = isdf_in%n_intp_r
  myn_intp_r = w_grp%myn_intp_r
  ldn_intp_r = w_grp%ldn_intp_r
  maxncv = isdf_in%maxncv
  maxnv = isdf_in%maxnv
  maxnc = isdf_in%maxnc

  ! Initialize h5 FORTRAN interface
  call H5open_f(h5err)

  ! create h5 file here
#ifdef DEBUG
  if (w_grp%master .and. verbose) write (6, *) "create h5 file"
#endif
  write (h5filename, '(A,I7.7,A)') "zeta_", w_grp%inode, ".h5"
  call H5Fcreate_f(h5filename, H5F_ACC_TRUNC_F, file_id, h5err)
  !
  ! each processor stores part of P, Q and zeta
  !
#ifdef DEBUG
  if (verbose) then
    do ipe = 0, w_grp%npes - 1
      if (w_grp%inode == ipe) then
        write (6, '(a)') " Index of interpolation points in full domain: "
        write (6, *) "n_intp_r", n_intp_r
        write (6, '(5i14)') (isdf_in%intp_r(ii), ii=1, n_intp_r)
      end if
      call MPI_BARRIER(w_grp%comm, errinfo)
    end do
  end if
#endif
  !
  ! isdf_in%ivlist(:) maps the index of valence states used in
  !   calculation (i.e., stored in memory) to the real index of valence states
  !ALLOCATE( inv_ivlist(isdf_in%maxivv, nspin, kpt%nk, gvec%syms%ntrans) ) 
  ! For now, only deal with confined system. So we assume kpt%nk=1, and there is no dependence on k here
  !ALLOCATE( inv_iclist(isdf_in%maxicc, nspin, kpt%nk, gvec%syms%ntrans) )

  ! inv_ivlist(:) maps the real index of valence states
  !   to the index of valence states used in the calculation
  !inv_ivlist = 0
  !inv_iclist = 0
  !
  !do ikp = 1, kpt%nk
  !  do isp = 1, nspin
  !    do irp = 1, gvec%syms%ntrans
  !      do iv = 1, isdf_in%nv(isp,ikp,irp)
  !        IVV = isdf_in%ivlist(iv, isp, ikp, irp)
  !        inv_ivlist(IVV, isp, ikp, irp) = iv
  !      enddo
  !      do ic = 1, isdf_in%nc(isp,ikp,irp)
  !        ICC = isdf_in%iclist(ic, isp, ikp, irp)
  !        inv_iclist(ICC, isp, ikp, irp) = ic
  !      enddo
  !     enddo
  !  enddo
  !enddo
  !
  ! qkt is set to zero. This is only valid for
  !  tests of nonperiodic system, and should be updated later.
  !
  qkt = 0
  !
  ! kflag = 0 : calculate kernel K^x  ( Coulomb )
  !         1 :                  K^x + K^f ( Coulomb + 1/2 * F_xc )
  !         2 :                  K^f   ( 1/2 * F_xc )
  !         3 :                  K^t   ( 1/2 * F_xc for spin triplet )
  !
  ! Generate Coulomb potential
  !
  if (kflag < 2) then
    call Zinitialize_FFT(w_grp%inode, fft_box)
    call Zcreate_coul_0D(gvec%bdot, qkt, fft_box)
    if (w_grp%master) write (*, *) " finished creating FFT plans and coul_0D"
  end if
  !
  ! Calculate LDA kernel
  !
  if (kflag > 0) then
    !
    allocate (fxc(w_grp%mydim, nspin, nspin), stat=errinfo)
    fxc = zero
    !
    ! Copy the charge density to fxc
    !
    do isp = 1, nspin
      call dcopy(w_grp%mydim, kpt%rho(w_grp%offset + 1, isp), 1, fxc(1, isp, 1), 1)
    end do
    call xc_init(nspin, XC_LDA_X, XC_LDA_C_PZ, 0, zero, one, .false., xc_lda)
    xc_lda%has_grad = .false.
    call fxc_get(xc_lda, nspin, w_grp%mydim, kflag, fxc)
    call xc_end(xc_lda)
    !
    write (*, *) "inode: ", w_grp%inode, ": finished initializing xc functional"
  end if
  !
  ! create dataspace and dataset here
  !
  if (kpt%lcplx) then
    n_cplx = 2
  else
    n_cplx = 1
  end if
  rank = 3
  data_dims(1) = w_grp%mydim
  data_dims(2) = n_intp_r
  data_dims(3) = n_cplx
  if (peinf%master) write (*, '(A,3(X,I0))') " hdf5_data_dims =", data_dims(1), data_dims(2), data_dims(3)
  ! create dataspace
#ifdef DEBUG
  if (w_grp%master) write (*, *) "Create dataspace: dspace_zeta"
#endif
  call H5Screate_simple_f(rank, data_dims, dspace_zeta, h5err)
  ! create dataspace for vc_zeta
#ifdef DEBUG
  if (w_grp%master) write (*, *) "Create dataspace: dspace_vczeta"
#endif
  call H5Screate_simple_f(rank, data_dims, dspace_vczeta, h5err)
  ! create hdf5 datasets for zeta and V_c*zeta for each spin and group representation
  do isp = 1, nspin
    ! zeta
    do jrp = 1, gvec%syms%ntrans
      write (dset_zeta, '(A,I1.1,A,I2.2)') "zeta_", isp, "_", jrp
      call H5Dcreate_f(file_id, dset_zeta, H5T_NATIVE_DOUBLE, dspace_zeta, dset_zeta_id(isp, jrp), h5err)
    end do  ! jrp
    ! V_c*zeta
    do jrp = 1, gvec%syms%ntrans/r_grp%num
      irp = r_grp%g_rep(jrp)
      write (dset_vczeta, '(A,I1.1,A,I2.2)') "vczeta_", isp, "_", irp
      call H5Dcreate_f(file_id, dset_vczeta, H5T_NATIVE_DOUBLE, dspace_vczeta, dset_vczeta_id(isp, irp), h5err)
    end do  ! jrp
  end do

  isdf_in%ZMmtrx_loc = Zzero
  ! Dimension: ZMmtrx_loc(w_grp%myn_intp_r, n_intp_r, nspin, nspin, kpt%nk, 2, gvec%syms%ntrans)
  norm_factor = 1.d0/gvec%hcub*gvec%syms%ntrans
  ! Here we assume there is only 1 r_grp and 1 w_grp
  call blacs_get(-1, 0, context_system)
  icntxt_1d = context_system
  nprow_1d = 1
  npcol_1d = w_grp%npes
  call blacs_gridinit(icntxt_1d, 'c', nprow_1d, npcol_1d)
  call blacs_gridinfo(icntxt_1d, nprow_1d, npcol_1d, myrow_1d, mycol_1d)
  if (w_grp%inode /= mycol_1d) then
    call die("isdf_parallel_sym_UtraLowMem: inode is not equal to mycol_1d")
  end if
  ! SUBROUTINE descinit(DESC, M, N, MB, NB, IRSRC, ICSRC, ICTXT, LLD, INFO)
  ! Initialize the descriptor of the Amtrx in the 1d process grid
  ! Note: scalapack requires desca(mb_) .eq. desca(nb_) .eq. descab(mb_)
  ! see source code
  if (w_grp%npes >= 2) then
    call descinit(desca_1d, n_intp_r, n_intp_r, w_grp%ldn_intp_r, w_grp%ldn_intp_r, 0, 0, icntxt_1d, n_intp_r, info)
    ! Initialize the descriptor of the Bmtrx in the 1d process grid
    call descinit(descb_1d, n_intp_r, gvec%nr, w_grp%ldn_intp_r, 1, 0, 0, icntxt_1d, n_intp_r, info)
  else
    call descinit(desca_1d, n_intp_r, n_intp_r, n_intp_r, n_intp_r, 0, 0, icntxt_1d, n_intp_r, info)
    ! Initialize the descriptor of the Bmtrx in the 1d process grid
    call descinit(descb_1d, n_intp_r, gvec%nr, n_intp_r, gvec%nr, 0, 0, icntxt_1d, n_intp_r, info)
  end if

  RSRC_ = 7
  MB_ = 5

  ! Information for 2D process grid, need for solving linear equation
  nprow_2d = int(sqrt(real(w_grp%npes) + 1e-6))
  do ii = nprow_2d, 1, -1
    if (mod(w_grp%npes, ii) == 0) exit
  end do
  nprow_2d = ii
  npcol_2d = w_grp%npes/nprow_2d
  nbl_2d = min(opt%pdgesv_nbl2d, n_intp_r/(max(nprow_2d, npcol_2d)))
  nbl_2d = max(nbl_2d, 1)
  if (peinf%master) print *, " pdgesv blocksize ", nbl_2d, "x", nbl_2d
  icntxt_2d = context_system
  ! Initialize the SL context for 1d process grid
  call blacs_gridinit(icntxt_2d, 'c', nprow_2d, npcol_2d)
  ! Get the row and col indices for the current process
  call blacs_gridinfo(icntxt_2d, nprow_2d, npcol_2d, myrow_2d, mycol_2d)
  ldrowA = numroc(n_intp_r, nbl_2d, myrow_2d, 0, nprow_2d)
  ldcolA = numroc(n_intp_r, nbl_2d, mycol_2d, 0, npcol_2d)
  ldrowB = numroc(n_intp_r, nbl_2d, myrow_2d, 0, nprow_2d)
  ldcolB = numroc(gvec%nr, nbl_2d, mycol_2d, 0, npcol_2d)
  if (myrow_2d /= -1) then
    allocate (Amtrx_bl(ldrowA*ldcolA))
    allocate (Bmtrx_bl(ldrowB*ldcolB))
    call descinit(desca_2d, n_intp_r, n_intp_r, nbl_2d, nbl_2d, 0, 0, icntxt_2d, ldrowA, info)
    call descinit(descb_2d, n_intp_r, gvec%nr, nbl_2d, nbl_2d, 0, 0, icntxt_2d, ldrowA, info)
  else
    desca_2d(2) = -1
    descb_2d(2) = -1
  end if

  ! Allocate tmp_array for hdf5 operations
  if (kpt%lcplx) then
    n_cplx = 2
  else
    n_cplx = 1
  end if
  if (w_grp%master) then
    write (*, '(A)') " Allocate matrices"
    write (*, '(A,F0.2,A)') "  tmp_array: ", 1.d0 * w_grp%mydim * isdf_in%n_intp_r * n_cplx / 2**17, " MB"
  end if
  allocate (tmp_array(w_grp%mydim, isdf_in%n_intp_r, n_cplx))

  ! assume kpt%wfn(isp,ikp)%nmem is the same for all isp and ikp
  if (w_grp%master) then
    write (*, '(A,F0.2,A)') "  tmp_Psi_intp_loc: ", 1.d0 * ldn_intp_r * kpt%wfn(1, 1)%nmem / 2**17, " MB"
    write (*, '(A,F0.2,A)') "  tmp_Mmtrx_loc: ", 1.d0 * ldn_intp_r * n_intp_r / 2**17, " MB"
  end if
  allocate (tmp_Psi_intp_loc(ldn_intp_r, kpt%wfn(1, 1)%nmem))
  allocate (tmp_Mmtrx_loc(ldn_intp_r, n_intp_r))
  tmp_Mmtrx_loc = 0.d0

  !
  ! compute zeta functions (interpolation vectors)
  !
  do ikp = 1, kpt%nk
    do isp = 1, nspin
#ifdef DEBUG
      if (w_grp%master) write (*, '(4(A,I0))') "  ikp: ", ikp, "/", kpt%nk, "  isp: ", isp, "/", nspin
#endif
      ! construct isdf%ZPsi_intp_loc
      call stopwatch(peinf%master, "Start distribute_intp_pts")
      call Zdistribute_intp_pts(kpt%wfn(isp, ikp), isdf_in, gvec%syms%ntrans, isp, ikp)
      call stopwatch(peinf%master, "Finish distribute_intp_pts")
      !do ipe = 0, w_grp%npes-1
      !  if (w_grp%inode .eq. ipe) then
      !    if (ipe .eq. 0) write(6, *) isp, ikp, " ZPsi_intp_loc "
      !    print *, " ipe = ", ipe
      !    call printmatrix( isdf_in%ZPsi_intp_loc(1,1,1,1), w_grp%myn_intp_r, &
      !    kpt%wfn(1,1)%nmem, 6 )
      !  endif
      !  call MPI_BARRIER(w_grp%comm, info)
      !enddo

      if (w_grp%master) then
        write (*, '(A)') " Allocate matrices"
        write (*, '(A,F0.2,A)') "  P: ", 1.d0 * n_intp_r * w_grp%mydim * gvec%syms%ntrans / 2**17, " MB"
        write (*, '(A,F0.2,A)') "  Q: ", 1.d0 * n_intp_r * w_grp%mydim * gvec%syms%ntrans / 2**17, " MB"
        write (*, '(A,F0.2,A)') "  P_intp: ", 1.d0 * n_intp_r * myn_intp_r * gvec%syms%ntrans / 2**17, " MB"
        write (*, '(A,F0.2,A)') "  Q_intp: ", 1.d0 * n_intp_r * myn_intp_r * gvec%syms%ntrans / 2**17, " MB"
        write (*, '(A,F0.2,A)') "  Amtrx: ", 1.d0 * n_intp_r * myn_intp_r * gvec%syms%ntrans / 2**17, " MB"
        write (*, '(A,F0.2,A)') "  Bmtrx: ", 1.d0 * n_intp_r * w_grp%mydim / 2**17, " MB"
        write (*, '(A,F0.2,A)') "  Xmtrx: ", 1.d0 * w_grp%mydim * n_intp_r / 2**17, " MB"
      end if
      allocate (P(n_intp_r, w_grp%mydim, gvec%syms%ntrans))
      allocate (Q(n_intp_r, w_grp%mydim, gvec%syms%ntrans))
      allocate (P_intp(n_intp_r, myn_intp_r, gvec%syms%ntrans))
      allocate (Q_intp(n_intp_r, myn_intp_r, gvec%syms%ntrans))
      allocate (Amtrx(n_intp_r, myn_intp_r, gvec%syms%ntrans))
      allocate (Bmtrx(n_intp_r, w_grp%mydim))
      allocate (Xmtrx(w_grp%mydim, n_intp_r))

      ! initialize matrices with zero
      P_intp = Zzero
      Q_intp = Zzero
      Amtrx = Zzero
      P = Zzero
      Q = Zzero

      call timacc(53, 1, tsec)

      ! The following loop calculate P(r,r_u,jrp), Q(r,r_u,jrp) for all representations
      !
      do jrp = 1, gvec%syms%ntrans
#ifdef DEBUG
        if (w_grp%master .and. verbose) then
          print *, " jrp = ", jrp
          write (dbgunit, *) ' isp = ', isp, ', ikp = ', ikp
        end if
#endif

        ! ldn_intp_r = max(myn_intp_r) among all the procs
        do ipe = 0, w_grp%npes - 1
          if (w_grp%inode == ipe) then
            tmp_Psi_intp_loc(1:myn_intp_r, 1:kpt%wfn(isp, ikp)%nmem) = &
              isdf_in%ZPsi_intp_loc(1:myn_intp_r, 1:kpt%wfn(isp, ikp)%nmem, isp, ikp)
          end if
          call MPI_BCAST(tmp_Psi_intp_loc(1, 1), ldn_intp_r*kpt%wfn(isp, ikp)%nmem, MPI_DOUBLE_SCALAR, ipe, &
                         w_grp%comm, info)
          call timacc(78, 1, tsec)
          do iv = 1, isdf_in%nv(isp, ikp, jrp)
            IVV = isdf_in%ivlist(iv, isp, ikp, jrp)
            JVV = kpt%wfn(isp, ikp)%map(IVV)
            ! Compute P(n_intp_r, w_grp%mydim)
            call Zgemm_hl('n', 't', w_grp%n_intp_end(ipe) - w_grp%n_intp_start(ipe) + 1, w_grp%mydim, 1, one, &
                          tmp_Psi_intp_loc(1, JVV), ldn_intp_r, kpt%wfn(isp, ikp)%Zwf(1, JVV), w_grp%ldn, one, &
                          P(w_grp%n_intp_start(ipe), 1, jrp), n_intp_r, opt%linear_algebra)
            call Zgemm_hl('n', 't', w_grp%n_intp_end(ipe) - w_grp%n_intp_start(ipe) + 1, myn_intp_r, 1, one, &
                          tmp_Psi_intp_loc(1, JVV), ldn_intp_r, isdf_in%ZPsi_intp_loc(1, JVV, isp, ikp), myn_intp_r, &
                          one, P_intp(w_grp%n_intp_start(ipe), 1, jrp), n_intp_r, opt%linear_algebra)
          end do  ! iv

          do ic = 1, isdf_in%nc(isp, ikp, jrp)
            ICC = isdf_in%iclist(ic, isp, ikp, jrp)
            JCC = kpt%wfn(isp, ikp)%map(ICC)
            ! Compute Q
            call Zgemm_hl('n', 't', w_grp%n_intp_end(ipe) - w_grp%n_intp_start(ipe) + 1, w_grp%mydim, 1, one, &
                          tmp_Psi_intp_loc(1, JCC), ldn_intp_r, kpt%wfn(isp, ikp)%Zwf(1, JCC), w_grp%ldn, one, &
                          Q(w_grp%n_intp_start(ipe), 1, jrp), n_intp_r, opt%linear_algebra)
            call Zgemm_hl('n', 't', w_grp%n_intp_end(ipe) - w_grp%n_intp_start(ipe) + 1, myn_intp_r, 1, one, &
                          tmp_Psi_intp_loc(1, JCC), ldn_intp_r, isdf_in%ZPsi_intp_loc(1, JCC, isp, ikp), myn_intp_r, &
                          one, Q_intp(w_grp%n_intp_start(ipe), 1, jrp), n_intp_r, opt%linear_algebra)
          end do  ! ic
          call timacc(78, 2, tsec)

        end do  ! ipe

      end do ! jrp
      call stopwatch(peinf%master, ' Finish constructing P, Q, P_intp, Q_intp. ')

      ! Calculate zeta(r,n_intp_r,jrp) for all representations
      do jrp = 1, gvec%syms%ntrans/r_grp%num
        if (w_grp%master) then
          write (*, '(A)') repeat('-', 65)
          write (*, '(2(A,I0))') "  jrp: ", jrp, "/", gvec%syms%ntrans/r_grp%num
        end if
        irp = r_grp%g_rep(jrp)
        Bmtrx = Zzero
        Xmtrx = Zzero
        ! Calculate A and B
        !
        ! Loop over all the reps.
        call timacc(80, 1, tsec)
        do lrp1 = 1, gvec%syms%ntrans
          ! The direct product of lrp1 and lrp2 should be irp: lrp1 * lrp2 = irp
          do lrp2 = 1, gvec%syms%ntrans
            if (gvec%syms%prod(lrp1, lrp2) == irp) exit
          end do
#ifdef DEBUG
          if (w_grp%master) then
            write (dbgunit, *) "lrp1 ", lrp1, ", lrp2 ", lrp2, ", irp ", irp
          end if
#endif
          !
          ! ---------------------
          ! For each irp, spin and ikp, calculate zeta
          ! zeta dimension: Ng*Nu
          ! Set A = ( sum_{lrp1} P_intp(r_u,r_u,lrp1).Q_intp(r_u,r_u,lrp2) )^T  dimension: n_intp_r * n_intp_r
          !     B = ( sum_{lrp1} P(r,r_u,lrp1).Q(r,r_u,lrp2) )^T                dimension: n_intp_r * w_grp%mydim
          ! A is a symmetric matrix!
          ! we solve the linear equation A*(zeta^T) = B to get zeta^T for representation irp
          ! zeta^T = A^-1 * B = (C * C^T)^-1 * C * Z^T
          !
          !  Matrix dimensions:
          !  zeta(ngfl, n_intp_r, :, :)
          !  Amtrx(n_intp_r, n_intp_r)     intermediate variable, store C * C^T
          !  Bmtrx(n_intp_r, w_grp%mydim)         intermediate variable, store C * Z^T
          !  Xmtrx(n_intp_r, w_grp%mydim)         intermediate variable, store zeta^T
          ! ---------------------
          !
          ! calculate A = P_intp * Q_intp (Note: This is an element-wise multipliation)
          !
          Amtrx(1:n_intp_r, 1:myn_intp_r, irp) = Amtrx(1:n_intp_r, 1:myn_intp_r, irp) + &
                                                 P_intp(1:n_intp_r, 1:myn_intp_r, lrp1)* &
                                                 Q_intp(1:n_intp_r, 1:myn_intp_r, lrp2)
          !
          ! calculate B = (P.Q)^T (Note: This is an element-wise multiplication)
          !
          Bmtrx(1:n_intp_r, 1:w_grp%mydim) = Bmtrx(1:n_intp_r, 1:w_grp%mydim) + &
                                             P(1:n_intp_r, 1:w_grp%mydim, lrp1)* &
                                             Q(1:n_intp_r, 1:w_grp%mydim, lrp2)
        end do ! lrp1
        call timacc(80, 2, tsec)
        !
        ! solve linear equation A * X = B
        !
        ! Note that dgesv will change Amtrx, so we need a temporary Amtrx1
        ! Amtrx1(1:n_intp_r,1:n_intp_r) = Amtrx(1:n_intp_r,1:n_intp_r,irp)
        ! call dgesv(isdf_in%n_intp_r, w_grp%mydim, Amtrx1(1,1), n_intp_r, ipiv, Bmtrx, n_intp_r, einfo)
        ! Use Scalapack to solve the linear system
        call MPI_BARRIER(w_grp%comm, errinfo)

        call stopwatch(w_grp%master, "Call pZgemr2d to redistribute Amtrx to 2D grid")
        call pZgemr2d(n_intp_r, n_intp_r, Amtrx(1, 1, irp), 1, 1, desca_1d, Amtrx_bl, 1, 1, desca_2d, icntxt_1d)

        call stopwatch(w_grp%master, "Call pZgemr2d to redistribute Bmtrx to 2D grid")
        call pZgemr2d(n_intp_r, gvec%nr, Bmtrx, 1, 1, descb_1d, Bmtrx_bl, 1, 1, descb_2d, icntxt_1d)

        call stopwatch(w_grp%master, "Call pZgesv")
        ndim_ipiv = numroc(n_intp_r, nbl_2d, myrow_2d, desca_2d(RSRC_), nprow_2d) + desca_2d(MB_)
        if (myrow_2d /= -1 .and. mycol_2d /= -1) then
          allocate (ipiv(ndim_ipiv))
          call pZgesv(n_intp_r, gvec%nr, Amtrx_bl, 1, 1, desca_2d, ipiv, Bmtrx_bl, 1, 1, descb_2d, info)
        end if

        call stopwatch(w_grp%master, "Call pZgemr2d to redistribute Amtrx to 1D grid")
        call pZgemr2d(n_intp_r, n_intp_r, Amtrx_bl, 1, 1, desca_2d, Amtrx(1, 1, irp), 1, 1, desca_1d, icntxt_1d)

        call stopwatch(w_grp%master, "Call pZgemr2d to redistribute Bmtrx to 1D grid")
        call pZgemr2d(n_intp_r, gvec%nr, Bmtrx_bl, 1, 1, descb_2d, Bmtrx, 1, 1, descb_1d, icntxt_1d)

        if (myrow_2d /= -1 .and. mycol_2d /= -1) deallocate (ipiv)
        Xmtrx(1:w_grp%mydim, 1:n_intp_r) = transpose(Bmtrx(1:n_intp_r, 1:w_grp%mydim)) 
        ! probably we can remove Bmtrx here now

#ifdef DEBUG
        if (.true. .and. w_grp%master) then
          write (dbgunit, '(" irp =", i3, "  Zeta = ")') irp
          call printmatrix(Xmtrx(1:w_grp%mydim, 1:isdf_in%n_intp_r), w_grp%mydim, isdf_in%n_intp_r, dbgunit)
        end if
#endif
        !
        ! Copy Xmtrx to zeta
        !
        ! Xmtrx (w_grp%mydim, n_intp_r)
#ifdef CPLX
        tmp_array(1:w_grp%mydim, 1:isdf_in%n_intp_r, 1) = real(Xmtrx(1:w_grp%mydim, 1:isdf_in%n_intp_r))
        tmp_array(1:w_grp%mydim, 1:isdf_in%n_intp_r, 2) = aimag(Xmtrx(1:w_grp%mydim, 1:isdf_in%n_intp_r))
#else
        tmp_array(1:w_grp%mydim, 1:isdf_in%n_intp_r, 1) = Xmtrx(1:w_grp%mydim, 1:isdf_in%n_intp_r)
#endif
        call H5Dwrite_f(dset_zeta_id(isp, irp), H5T_NATIVE_DOUBLE, tmp_array, data_dims, h5err)
      end do  ! jrp = 1, gvec%syms%ntrans/r_grp%num
    end do  ! isp
    call MPI_BARRIER(peinf%comm, errinfo)

    call stopwatch(peinf%master, " deallocate arrays")
    deallocate (P)
    deallocate (Q)
    deallocate (P_intp)
    deallocate (Q_intP)
    deallocate (Amtrx)
    deallocate (Bmtrx)
    deallocate (Xmtrx)

    call timacc(53, 2, tsec)

    call timacc(54, 1, tsec)

    call stopwatch(peinf%master, " start to calculate Mmtrx")
    if (kflag < 2) then
      !
      allocate (rho_h(gvec%nr))  ! gvec%nr=w_grp%nr, the number of RS grid points in reduced RS domain
      allocate (rho_h_distr(w_grp%ldn, w_grp%npes))
      do jrp = 1, gvec%syms%ntrans/r_grp%num
        ! jrp is the index of representation belong to this r_grp
        ! irp is the real index of the representation
        !
        irp = r_grp%g_rep(jrp)
        !
        ! Now calculate <zeta_u(r,ispin)|V(r,r')|zeta_w(r',jspin)>, where u, w = 1, ..., n_intp
        ! and store it in Mmtrx(n_intp_r, n_intp_r)
        !
        do isp = 1, nspin
          !
          ! Exp: n_intp_r=1000, r_grp%npes=10, w_grp%npes=5
          !      then w_grp%mygr=0 work on ii = 1,2,3,4,5,  11,12,13,14,15, 21,22,23,24,25, ...
          !           w_grp%mygr=1 work on ii = 6,7,8,9,10, 16,17,18,19,20, 26,27,28,29,30, ...
          !
          ! create a new subdataspace
          rank = 3
          subdim(1) = w_grp%mydim
          subdim(2) = w_grp%npes
          subdim(3) = n_cplx
          call H5Screate_simple_f(rank, subdim, subdspace, h5err)
          ! Warning: currently, only works for: 1 w_grp per each r_grp
          !          this means r_grp%npes == w_grp%npes
          do i_row = 1, isdf_in%n_intp_r, r_grp%npes
            !
            ! locate a slice of dataset
            !
            shift(1) = 0
            shift(2) = w_grp%mygr*w_grp%npes + i_row - 1
            shift(3) = 0
            if (shift(2) + 1 > isdf_in%n_intp_r) cycle
            if (shift(2) + subdim(2) > isdf_in%n_intp_r) then
              subdim(2) = isdf_in%n_intp_r - shift(2)
              call H5Sclose_f(subdspace, h5err)
              call H5Screate_simple_f(rank, subdim, subdspace, h5err)
            end if
            !
            ! read rho_h_distr from hdf5 file
            !
            stride = (/1, 1, 1/)
            block = (/1, 1, 1/)
            call H5Sselect_hyperslab_f(dspace_zeta, H5S_SELECT_SET_F, shift, subdim, h5err, stride, block)
            call H5Dread_f(dset_zeta_id(isp, irp), H5T_NATIVE_DOUBLE, tmp_array(1:subdim(1), 1:subdim(2), :), &
                           data_dims, h5err, subdspace, dspace_zeta)
#ifdef CPLX
            rho_h_distr(1:subdim(1), 1:subdim(2)) = cmplx(tmp_array(1:subdim(1), 1:subdim(2), 1), &
                                                          tmp_array(1:subdim(1), 1:subdim(2), 2), kind=dpc)
#else
            rho_h_distr(1:subdim(1), 1:subdim(2)) = tmp_array(1:subdim(1), 1:subdim(2), 1)
#endif
            !
            ! initialize rho_h
            !
            rho_h = Zzero
            !
            ii = w_grp%mygr*w_grp%npes + i_row + w_grp%inode
            call Zgather(1, rho_h_distr, rho_h)
            if (ii <= isdf_in%n_intp_r) then
              !
              ! solve poisson equation to get: rho_h(r) = \int V_c(r,r') zeta_ii(r') dr'
              !
              call timacc(79, 1, tsec)
              call Zpoisson(gvec, rho_h, irp)
              call timacc(79, 2, tsec)
            end if
            call Zscatter(1, rho_h_distr, rho_h)
            !
            ! subdim(1) = w_grp%mydim is the leading dimension of tmp_array
#ifdef CPLX
            tmp_array(1:subdim(1), 1:subdim(2), 1) = real(rho_h_distr(1:subdim(1), 1:subdim(2)))
            tmp_array(1:subdim(1), 1:subdim(2), 2) = aimag(rho_h_distr(1:subdim(1), 1:subdim(2)))
#else
            tmp_array(1:subdim(1), 1:subdim(2), 1) = rho_h_distr(1:subdim(1), 1:subdim(2))
#endif
            call H5Sselect_hyperslab_f(dspace_vczeta, H5S_SELECT_SET_F, shift, subdim, h5err, stride, block)
            call H5Dwrite_f(dset_vczeta_id(isp, irp), H5T_NATIVE_DOUBLE, &
                            tmp_array(1:subdim(1), 1:subdim(2), 1:n_cplx), data_dims, h5err, subdspace, dspace_vczeta)
          end do ! i_row
        end do ! isp
      end do ! jrp

      deallocate (rho_h)
      deallocate (rho_h_distr)

    end if ! kflag < 2

    call stopwatch(peinf%master, " allocate zeta")
    allocate (zeta(w_grp%mydim, isdf_in%n_intp_r, nspin))
    if (kflag > 0) allocate (fzeta(w_grp%mydim, isdf_in%n_intp_r))
    if (kflag < 2) allocate (vzeta(w_grp%mydim, isdf_in%n_intp_r, nspin))

    do jrp = 1, gvec%syms%ntrans/r_grp%num
      irp = r_grp%g_rep(jrp)
      do isp = 1, nspin
        ! open dataspace
        call H5Sclose_f(dspace_zeta, h5err)
        call H5Dget_space_f(dset_zeta_id(isp, irp), dspace_zeta, h5err)
        if (kflag < 2) then
          call H5Sclose_f(dspace_vczeta, h5err)
          call H5Dget_space_f(dset_vczeta_id(isp, irp), dspace_vczeta, h5err)
        end if
      end do

      do isp = 1, nspin
        call H5Dread_f(dset_zeta_id(isp, irp), H5T_NATIVE_DOUBLE, tmp_array, data_dims, h5err)
#ifdef CPLX
        zeta(1:w_grp%mydim, 1:isdf_in%n_intp_r, isp) = cmplx(tmp_array(1:w_grp%mydim, 1:isdf_in%n_intp_r, 1), &
                                                             tmp_array(1:w_grp%mydim, 1:isdf_in%n_intp_r, 2), &
                                                             kind=dpc)
#else
        zeta(1:w_grp%mydim, 1:isdf_in%n_intp_r, isp) = tmp_array(1:w_grp%mydim, 1:isdf_in%n_intp_r, 1)
#endif

        if (kflag < 2) then
          call H5Dread_f(dset_vczeta_id(isp, irp), H5T_NATIVE_DOUBLE, tmp_array, data_dims, h5err)
#ifdef CPLX
          vzeta(1:w_grp%mydim, 1:isdf_in%n_intp_r, isp) = cmplx(tmp_array(1:w_grp%mydim, 1:isdf_in%n_intp_r, 1), &
                                                                tmp_array(1:w_grp%mydim, 1:isdf_in%n_intp_r, 2), &
                                                                kind=dpc)
#else
          vzeta(1:w_grp%mydim, 1:isdf_in%n_intp_r, isp) = tmp_array(1:w_grp%mydim, 1:isdf_in%n_intp_r, 1)
#endif
        end if
      end do  ! isp

      ! calculate mtrx
      do isp = 1, nspin
        do csp = 1, nspin
          do ipe = 0, w_grp%npes - 1
            if (kflag > 0) then
              ! TODO: check if this is correct for kflag > 0
              do ii = 1, isdf_in%n_intp_r
                fzeta(1:w_grp%mydim, ii) = fxc(1:w_grp%mydim, isp, csp) * zeta(1:w_grp%mydim, ii, csp)
              end do ! ii
#ifdef DEBUG
              if (w_grp%master) then
                write (dbgunit, *) " test zeta fzeta irp =", irp
                do jj = 1, 10
                  write (dbgunit, *) jj - 1, zeta(jj, 1, 1), fzeta(jj, 1)
                end do
              end if
#endif

              n_row = w_grp%n_intp_end(ipe) - w_grp%n_intp_start(ipe) + 1
              ! TODO: check if a conjugate is needed here
              ! old code
              ! call dgemm('T', 'N', n_row, n_intp_r, w_grp%mydim, norm_factor, zeta(1,w_grp%n_intp_start(ipe),isp), &
              !            w_grp%mydim, fzeta(1,1), w_grp%mydim, 1.d0, tmp_Mmtrx_loc(1, 1), ldn_intp_r)
              call timacc(77, 1, tsec)
              call Zgemm_hl('T', 'N', n_row, n_intp_r, w_grp%mydim, norm_factor, &
                            zeta(1, w_grp%n_intp_start(ipe), isp), w_grp%mydim, fzeta(1, 1), w_grp%mydim, 1.d0, &
                            tmp_Mmtrx_loc(1, 1), ldn_intp_r, opt%linear_algebra)
              call timacc(77, 2, tsec)
              call MPI_REDUCE(MPI_IN_PLACE, tmp_Mmtrx_loc(1, 1), n_row*n_intp_r, MPI_DOUBLE_SCALAR, MPI_SUM, ipe, &
                              w_grp%comm, errinfo)
              if (w_grp%inode == ipe) then
                isdf_in%ZMmtrx_loc(1:n_row, 1:n_intp_r, isp, csp, ikp, 2, irp) = tmp_Mmtrx_loc(1:n_row, 1:n_intp_r)
              end if
            end if ! kflag > 0
            if (kflag < 2) then
#ifdef DEBUG
              if (w_grp%master) then
                write (dbgunit, *) " test zeta vzeta irp =", irp
                do jj = 1, 10
                  write (dbgunit, *) jj - 1, zeta(jj, 1, 1), vzeta(jj, 1, 1)
                end do
              end if
#endif

              n_row = w_grp%n_intp_end(ipe) - w_grp%n_intp_start(ipe) + 1
              ! old code
              ! call dgemm('T', 'N', n_row, n_intp_r, w_grp%mydim, norm_factor, zeta(1,w_grp%n_intp_start(ipe),isp), &
              !            w_grp%mydim, vzeta(1,1,csp), w_grp%mydim, 0.d0, tmp_Mmtrx_loc(1, 1), ldn_intp_r)
              ! new code
              call timacc(77, 1, tsec)
              call Zgemm_hl('T', 'N', n_row, n_intp_r, w_grp%mydim, norm_factor, &
                            zeta(1, w_grp%n_intp_start(ipe), isp), w_grp%mydim, vzeta(1, 1, csp), w_grp%mydim, 0.d0, &
                            tmp_Mmtrx_loc(1, 1), ldn_intp_r, opt%linear_algebra)
              call timacc(77, 2, tsec)
              call MPI_REDUCE(tmp_Mmtrx_loc(1, 1), isdf_in%ZMmtrx_loc(1, 1, isp, csp, ikp, 1, irp), n_row*n_intp_r, &
                              MPI_DOUBLE_SCALAR, MPI_SUM, ipe, w_grp%comm, errinfo)
            end if  ! kflag < 2
          end do  ! ipe
        end do  ! csp
      end do  ! isp
    end do  ! jrp
    call timacc(54, 2, tsec)

    deallocate (zeta)
    if (kflag > 0) deallocate (fzeta)
    if (kflag < 2) deallocate (vzeta)
  end do ! ikp

  if (myrow_2d /= -1) then
    call blacs_gridexit(icntxt_2d)
    deallocate (Amtrx_bl)
    deallocate (Bmtrx_bl)
  end if
  if (myrow_1d /= -1) call blacs_gridexit(icntxt_1d)
  ! close dataspaces
  call H5Sclose_f(dspace_zeta, h5err)
  call H5Sclose_f(dspace_vczeta, h5err)
  call H5Sclose_f(subdspace, h5err)
  ! close dataset
  do isp = 1, nspin
    do jrp = 1, gvec%syms%ntrans
      call H5Dclose_f(dset_zeta_id(isp, jrp), h5err)
    end do
    do jrp = 1, gvec%syms%ntrans/r_grp%num
      irp = r_grp%g_rep(jrp)
      call H5Dclose_f(dset_vczeta_id(isp, irp), h5err)
    end do ! jrp
  end do
  ! close file
  call H5Fclose_f(file_id, h5err)
  ! close FORTRAN interface
  call H5close_f(h5err)

#ifndef DEBUG
  call execute_command_line("rm "//trim(h5filename))
#endif
  !
  ! clean up all the ALLOCATEd variables
  !
  !DEALLOCATE(inv_ivlist)
  !DEALLOCATE(inv_iclist)
  if (kflag > 0) then
    deallocate (fxc)
  end if

#ifdef DEBUG
  if (w_grp%master .and. .true.) then
    do jrp = 1, gvec%syms%ntrans
      write (dbgunit, '(a,i2,a)') " ZMmtrx_loc (:, :, isp=1, csp=1, ikp=1, 1, jrp=", jrp, ") = "
      call printmatrix(isdf_in%ZMmtrx_loc(1, 1, 1, 1, 1, 1, jrp), w_grp%myn_intp_r, n_intp_r, dbgunit)
    end do
    do jrp = 1, gvec%syms%ntrans
      write (dbgunit, '(a,i2,a)') " ZMmtrx_loc (:, :, isp=1, csp=1, ikp=1, 2, jrp=", jrp, ") = "
      call printmatrix(isdf_in%ZMmtrx_loc(1, 1, 1, 1, 1, 2, jrp), w_grp%myn_intp_r, n_intp_r, dbgunit)
    end do
  end if
#endif
  deallocate (tmp_Mmtrx_loc)
  deallocate (tmp_Psi_intp_loc)
  deallocate (tmp_array)
#ifdef DEBUG
  if (w_grp%master) then
    close (dbgunit)
  end if
#endif
  return

end subroutine Zisdf_parallel_sym_UltraLowMem

! prepare isdf%ZPsi_intp_loc
subroutine Zdistribute_intp_pts(wfn, isdf_in, ntrans, isp, ikp)

  use mpi_module
  use typedefs
  implicit none
#ifdef MPI
  include 'mpif.h'
#endif
  type(ISDF), intent(inout) :: isdf_in
  type(wavefunction), intent(in) :: wfn
  integer, intent(in) :: ntrans, isp, ikp
  SCALAR :: dummy(wfn%nmem)
  integer, parameter :: nchecks = 1000
  integer :: inode_dest, inode_source, ipt, iptf, iptr, ii, jj, mpi_err, tag
  integer(KIND=MPI_ADDRESS_KIND) :: max_tag
  logical :: flag
  integer :: ioff1(0:w_grp%npes - 1), ioff2(0:w_grp%npes - 1), mpistatus(MPI_STATUS_SIZE)

  call MPI_Comm_get_attr(w_grp%comm, MPI_TAG_UB, max_tag, flag, mpi_err)
  if (w_grp%master) print *, "MPI MAX_TAG = ", MAX_TAG
  ioff1 = 0
  ioff2 = 0
  ioff1(w_grp%inode) = w_grp%offset + 1
  ioff2(w_grp%inode) = w_grp%offset + w_grp%mydim
  call MPI_ALLREDUCE(MPI_IN_PLACE, ioff1, w_grp%npes, MPI_INTEGER, MPI_SUM, w_grp%comm, mpi_err)
  call MPI_ALLREDUCE(MPI_IN_PLACE, ioff2, w_grp%npes, MPI_INTEGER, MPI_SUM, w_grp%comm, mpi_err)
  ! if (w_grp%master) then
  !   print *, " ioff1 ", ioff1(0:w_grp%npes-1)
  !   print *, " ioff2 ", ioff2(0:w_grp%npes-1)
  ! endif
  inode_dest = 0 ! Start from the first proc
  ! print *, "w_grp%ldn_intp_r", w_grp%ldn_intp_r
  ! print *
  do ipt = 1, isdf_in%n_intp_r
    ! if (ipt == 1)  print *, " w_grp%n_intp_end(inode_dest) ", w_grp%n_intp_end(inode_dest)
    if (ipt > w_grp%n_intp_end(inode_dest)) then
      inode_dest = inode_dest + 1 ! update the destination proc
    end if
    ! determine whether this interpolation point
    ! is located in the current process
    iptf = isdf_in%intp_r(ipt)
    if (ntrans > 1) then
      iptr = iptf/ntrans + 1
    else
      iptr = iptf
    end if
    inode_source = iptr/(w_grp%ldn - 1)
    ! make sure the starting guess of inode_source is not out-of-bound
    do ii = 1, nchecks
      if (inode_source >= w_grp%npes - 1) then
        inode_source = inode_source - 1
      else
        exit
      end if
    end do
    ! look for the inode_source
    do ii = 1, nchecks
      if (iptr > ioff2(inode_source)) then
        inode_source = inode_source + 1
        continue
      end if
      if (iptr < ioff1(inode_source)) then
        inode_source = inode_source - 1
        continue
      end if
      if (inode_source > w_grp%npes - 1 .or. inode_source < 0) then
        if (w_grp%master) write (6, *) " ipt ", ipt, " iptf ", iptf, " iptr ", iptr, " ioff2(w_grp%npes-1) ", &
          ioff2(w_grp%npes - 1)
        call die("iptr is out of bound, see sigma.out or tdlda.out.")
      end if
      if (iptr >= ioff1(inode_source) .and. iptr <= ioff2(inode_source)) then
        exit
      end if
    end do
    if (w_grp%inode == inode_source) then
      ! This interpolation point is located in the
      ! current process
      jj = iptr - ioff1(inode_source) + 1
      if (inode_source == inode_dest) then
        ! w_grp%inode .eq. inode_dest
        isdf_in%ZPsi_intp_loc(ipt - w_grp%n_intp_start(inode_dest) + 1, 1:wfn%nmem, isp, ikp) = wfn%Zwf(jj, 1:wfn%nmem)
      else ! source and dest process are different
        dummy(1:wfn%nmem) = wfn%Zwf(jj, 1:wfn%nmem)
        tag = ipt
        call MPI_SEND(dummy(1), wfn%nmem, MPI_DOUBLE_SCALAR, inode_dest, tag, w_grp%comm, mpi_err)
        ! print *, mpi_err
      end if
    end if
    if (w_grp%inode == inode_dest .and. inode_dest /= inode_source) then
      ! if the current process is the destination process
      ! and the source process is not the destination process
      tag = ipt
      call MPI_RECV(dummy(1), wfn%nmem, MPI_DOUBLE_SCALAR, inode_source, tag, w_grp%comm, mpistatus, mpi_err)
      ! print *, mpi_err
      isdf_in%ZPsi_intp_loc(ipt - w_grp%n_intp_start(inode_dest) + 1, 1:wfn%nmem, isp, ikp) = dummy(1:wfn%nmem)
    end if
  end do ! ipt

end subroutine

