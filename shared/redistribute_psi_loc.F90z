#include "../shared/mycomplex.h"

subroutine Zredistribute_Psi_loc(isdf_in, nval, ncond)

  use typedefs
  use mpi_module
  implicit none
#ifdef MPI
  include 'mpif.h'
#endif

  integer, intent(in) :: nval, ncond
  type(ISDF), intent(inout) :: isdf_in
  integer :: ldn_intp_r, mpistatus(MPI_STATUS_SIZE), xvgrp, xcgrp, myvgrp, mycgrp, mynv, mync, ii, ldv, ldc, &
             myvstart, myvend, mycstart, mycend, xvgrp_c, igrp, icgrp, ivgrp, ipe, send_node, recv_node, tag, mpi_err, &
             incr, res, i1, i2, j1, j2
  SCALAR, allocatable :: tmparray(:)
  integer, allocatable :: vstart(:), cstart(:), vend(:), cend(:)

  ldn_intp_r = w_grp%ldn_intp_r
  xvgrp = int(sqrt(real(w_grp%npes*nval/ncond) + 1e-6))
  if (xvgrp <= 1) then
    xvgrp = 1
    xcgrp = w_grp%npes/xvgrp
  else  ! xvgrp > 1
    do ii = 1, xvgrp - 1
      xvgrp_c = xvgrp + ii
      if (mod(w_grp%npes, xvgrp_c) == 0) then
        xvgrp = xvgrp_c
        exit
      end if
      xvgrp_c = xvgrp - ii
      if (mod(w_grp%npes, xvgrp_c) == 0) then
        xvgrp = xvgrp_c
        exit
      end if
    end do
    xcgrp = w_grp%npes/xvgrp
    if (xcgrp*xvgrp /= w_grp%npes) then
      call die(" xcgrp * xvgrp \= w_grp%npes ")
    end if
  end if
  if (peinf%master) print *, " xcgrp = ", xcgrp, " xvgrp = ", xvgrp, " w_grp%npes ", w_grp%npes
  ! An example of distributing 24 process to 4 vgrp and 6 cgrp
  !___|__0___1___2___3____ xvgrp = 4
  ! 0 |  0   1   2   3   |
  ! 1 |  4   5   6   7   |
  ! 2 |  8   9   10  11  |
  ! 3 |  12  13  14  15  |
  ! 4 |  16  17  18  19  |
  !_5_|__20__21__22__23__|
  ! xcgrp = 6
  !
  myvgrp = mod(w_grp%inode, xvgrp)
  mycgrp = w_grp%inode/xvgrp
  !do ii = 0, 30
  !  if (ii .gt. w_grp%npes-1) exit
  !  if (w_grp%inode .eq. ii) then
  !    print *, " w_grp%inode ", w_grp%inode, myvgrp, mycgrp
  !  endif
  !enddo
  allocate (vstart(0:(xvgrp - 1)))
  allocate (vend(0:(xvgrp - 1)))
  allocate (cstart(0:(xcgrp - 1)))
  allocate (cend(0:(xcgrp - 1)))
  !
  ! distribute nval to xvgrp
  incr = nval/xvgrp
  res = mod(nval, xvgrp)
  if (res > 0) then
    ldv = incr + 1
  else
    ldv = incr
  end if
  do igrp = 0, xvgrp - 1
    if (igrp < res) then
      vstart(igrp) = igrp*(incr + 1) + 1
      vend(igrp) = igrp*(incr + 1) + incr + 1
    else
      vstart(igrp) = res*(incr + 1) + (igrp - res)*incr + 1
      vend(igrp) = res*(incr + 1) + (igrp - res)*incr + incr
    end if
  end do
  myvstart = vstart(myvgrp)
  myvend = vend(myvgrp)
  mynv = vend(myvgrp) - vstart(myvgrp) + 1
  ! distribute ncond to xcgrp
  incr = ncond/xcgrp
  res = mod(ncond, xcgrp)
  if (res > 0) then
    ldc = incr + 1
  else
    ldc = incr
  end if
  do igrp = 0, xcgrp - 1
    if (igrp < res) then
      cstart(igrp) = igrp*(incr + 1) + 1
      cend(igrp) = igrp*(incr + 1) + incr + 1
    else
      cstart(igrp) = res*(incr + 1) + (igrp - res)*incr + 1
      cend(igrp) = res*(incr + 1) + (igrp - res)*incr + incr
    end if
  end do
  mycstart = cstart(mycgrp)
  mycend = cend(mycgrp)
  mync = cend(mycgrp) - cstart(mycgrp) + 1
  !do ii = 0, 30
  !  if (ii .gt. w_grp%npes-1) exit
  !  if (w_grp%inode .eq. ii) then
  !    print *, " w_grp%inode ", w_grp%inode, myvstart, myvend, &
  !      mycstart, mycend
  !  endif
  !enddo
  call  MPI_BARRIER(w_grp%comm, mpi_err)
  allocate (isdf_in%ZPsiV_intp_bl(isdf_in%n_intp_r, mynv, 1, 1))
  allocate (isdf_in%ZPsiC_intp_bl(isdf_in%n_intp_r, mync, 1, 1))
  isdf_in%ZPsiV_intp_bl = Zzero
  isdf_in%ZPsiC_intp_bl = Zzero
  ! redistribute to ZPsiV_intp_bl
  allocate (tmparray(ldn_intp_r * ldv))
  do ipe = 0, w_grp%npes - 1
    send_node = ipe
    j1 = w_grp%n_intp_start(ipe)
    j2 = w_grp%n_intp_end(ipe)
    !if (w_grp%master) print *, " ipe ", ipe, " j1 ", j1, " j2 ", j2
    do ivgrp = 0, xvgrp - 1
      i1 = vstart(ivgrp)
      i2 = vend(ivgrp)
      !if (w_grp%master) print *, " i1 ", i1, " i2 ", i2
      do icgrp = 0, xcgrp - 1
        recv_node = icgrp*xvgrp + ivgrp
        tag = recv_node
        !if (w_grp%master) print *, " send_node ", send_node, " recv_node ", recv_node
        if (send_node /= recv_node) then
          !print *, " inode ", w_grp%inode, " comm "
          if (w_grp%inode == send_node) then
            call MPI_SEND(isdf_in%ZPsi_intp_loc(1, i1, 1, 1), (j2 - j1 + 1)*(i2 - i1 + 1), MPI_DOUBLE_SCALAR, &
                          recv_node, tag, w_grp%comm, mpi_err)
          end if
          if (w_grp%inode == recv_node) then
            call MPI_RECV(tmparray, (j2 - j1 + 1)*(i2 - i1 + 1), MPI_DOUBLE_SCALAR, send_node, tag, w_grp%comm, &
                          mpistatus, mpi_err)
            isdf_in%ZPsiV_intp_bl(j1:j2, 1:(i2 - i1 + 1), 1, 1) = &
              reshape(tmparray(1:(j2 - j1 + 1)*(i2 - i1 + 1)), (/j2 - j1 + 1, i2 - i1 + 1/))
          end if
        else ! send_node .eq. recv_node
          !print *, " inode ", w_grp%inode, " copy "
          if (w_grp%inode == recv_node) then
            isdf_in%ZPsiV_intp_bl(j1:j2, 1:(i2 - i1 + 1), 1, 1) = isdf_in%ZPsi_intp_loc(1:(j2 - j1 + 1), i1:i2, 1, 1)
          end if
        end if
        !if (w_grp%inode .eq. recv_node) then
        !  print *, "ZPsiV_intp_bl ", w_grp%inode, " recv "
        !  call printmatrix(isdf_in%ZPsiV_intp_bl(1,1,1,1), isdf_in%n_intp_r, mynv, 6)
        !endif
        !call MPI_BARRIER(w_grp%comm, mpi_err)
      end do ! icgrp
    end do ! ivgrp
  end do ! ipe
  deallocate (tmparray)
  ! if (w_grp%master) print *, " finish redistribut ZPsiV_intp_bl "
  ! do ipe = 0, w_grp%npes-1
  ! if (w_grp%inode .eq. ipe) then
  !   print *, "ZPsiV_intp_bl ", ipe
  !   call printmatrix(isdf_in%ZPsiV_intp_bl(1,1,1,1), isdf_in%n_intp_r, mynv, 6)
  ! endif
  ! call MPI_BARRIER(w_grp%comm, mpi_err)
  ! enddo
  ! do ipe = 0, w_grp%npes-1
  ! if (w_grp%inode .eq. ipe) then
  !   print *, "ZPsi_intp_loc", ipe
  !   call printmatrix(isdf_in%ZPsi_intp_loc(1,1,1,1), w_grp%myn_intp_r, nval, 6)
  ! endif
  ! call MPI_BARRIER(w_grp%comm, mpi_err)
  ! enddo

  allocate (tmparray(ldn_intp_r*ldc))
  do ipe = 0, w_grp%npes - 1
    send_node = ipe
    j1 = w_grp%n_intp_start(ipe)
    j2 = w_grp%n_intp_end(ipe)
    !if (w_grp%master) print *, " ipe ", ipe, " j1 ", j1, " j2 ", j2
    do icgrp = 0, xcgrp - 1
      i1 = cstart(icgrp)
      i2 = cend(icgrp)
      !if (w_grp%master) print *, " i1 ", i1, " i2 ", i2
      do ivgrp = 0, xvgrp - 1
        recv_node = icgrp*xvgrp + ivgrp
        tag = recv_node
        !if (w_grp%master) print *, " send_node ", send_node, " recv_node ", recv_node
        if (send_node /= recv_node) then
          !print *, " inode ", w_grp%inode, " comm "
          if (w_grp%inode == send_node) then
            call MPI_SEND(isdf_in%ZPsi_intp_loc(1, nval + i1, 1, 1), (j2 - j1 + 1)*(i2 - i1 + 1), MPI_DOUBLE_SCALAR, &
                          recv_node, tag, w_grp%comm, mpi_err)
          end if
          if (w_grp%inode == recv_node) then
            call MPI_RECV(tmparray, (j2 - j1 + 1)*(i2 - i1 + 1), MPI_DOUBLE_SCALAR, send_node, tag, w_grp%comm, &
                          mpistatus, mpi_err)
            isdf_in%ZPsiC_intp_bl(j1:j2, 1:(i2 - i1 + 1), 1, 1) = &
              reshape(tmparray(1:(j2 - j1 + 1)*(i2 - i1 + 1)), (/j2 - j1 + 1, i2 - i1 + 1/))
          end if
        else ! send_node .eq. recv_node
          !print *, " inode ", w_grp%inode, " copy "
          if (w_grp%inode == recv_node) then
            isdf_in%ZPsiC_intp_bl(j1:j2, 1:(i2 - i1 + 1), 1, 1) = &
              isdf_in%ZPsi_intp_loc(1:(j2 - j1 + 1), (nval + i1):(nval + i2), 1, 1)
          end if
        end if
        !if (w_grp%inode .eq. recv_node) then
        !  print *, "ZPsiC_intp_bl ", w_grp%inode, " recv "
        !  call printmatrix(isdf_in%ZPsiC_intp_bl(1,1,1,1), isdf_in%n_intp_r, mync, 6)
        !endif
        !call MPI_BARRIER(w_grp%comm, mpi_err)
      end do ! icgrp
    end do ! ivgrp
  end do ! ipe
  deallocate (tmparray)
  deallocate (vstart)
  deallocate (cstart)
  deallocate (vend)
  deallocate (cend)
  !if (w_grp%master) print *, " finish redistribut ZPsiC_intp_bl "
  !do ipe = 0, w_grp%npes-1
  !if (w_grp%inode .eq. ipe) then
  !  print *, "ZPsiC_intp_bl ", ipe
  !  call printmatrix(isdf_in%ZPsiC_intp_bl(1,1,1,1), isdf_in%n_intp_r, mync, 6)
  !endif
  !call MPI_BARRIER(w_grp%comm, mpi_err)
  !enddo
  !do ipe = 0, w_grp%npes-1
  !if (w_grp%inode .eq. ipe) then
  !  print *, "ZPsi_intp_loc", ipe
  !  call printmatrix(isdf_in%ZPsi_intp_loc(1,nval+1,1,1), w_grp%myn_intp_r, ncond, 6)
  !endif
  !call MPI_BARRIER(w_grp%comm, mpi_err)
  !enddo

  isdf_in%xvgrp = xvgrp
  isdf_in%xcgrp = xcgrp
  isdf_in%myvgrp = myvgrp
  isdf_in%mycgrp = mycgrp
  isdf_in%mynv = mynv
  isdf_in%mync = mync
  isdf_in%ldc = ldc
  isdf_in%ldv = ldv
  isdf_in%myvstart = myvstart
  isdf_in%myvend = myvend
  isdf_in%mycstart = mycstart
  isdf_in%mycend = mycend

end subroutine Zredistribute_Psi_loc

